{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si8b-5dUAw7D",
        "outputId": "fc2c3448-1018-42c5-b6a0-c569c7afe9d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.5.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.9.0+cu126)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.12/dist-packages (from opacus) (1.16.3)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->opacus) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.3)\n",
            "Downloading opacus-1.5.4-py3-none-any.whl (254 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/254.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.4/254.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opacus\n",
            "Successfully installed opacus-1.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GDG8EPN8vXbz",
        "outputId": "11f422f7-1818-4618-e241-313ca11ce95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DP-EGGROLL: Experimental Validation (Aligned with LaTeX)\n",
            "======================================================================\n",
            "\n",
            "Configuration:\n",
            "  Device: cuda\n",
            "  Nominal batch size m: 256\n",
            "  Target δ: 1e-05\n",
            "  Target epsilons: [0.5, 1.0, 2.0, 4.0, 8.0]\n",
            "  Epochs: 10\n",
            "  Seeds: [42, 101, 999]\n",
            "  Dataset size n: 60000\n",
            "  Sample rate q = m/n: 0.004267\n",
            "  Steps per epoch: 235\n",
            "  Total steps T: 2350\n",
            "\n",
            "  Hyperparameters:\n",
            "    Population size N: 8192\n",
            "    ES noise σ: 0.03\n",
            "    Learning rate α: 0.02\n",
            "    Clipping norm C: 4.0\n",
            "    Sensitivity Δ_2 = C/m: 0.015625\n",
            "\n",
            "======================================================================\n",
            "NON-PRIVATE EGGROLL BASELINE\n",
            "======================================================================\n",
            "  Seed 42: 95.55%\n",
            "  Seed 101: 95.80%\n",
            "  Seed 999: 95.91%\n",
            "\n",
            "  Non-Private EGGROLL: 95.75 ± 0.15%\n",
            "\n",
            "======================================================================\n",
            "TARGET EPSILON = 0.5\n",
            "======================================================================\n",
            "  Calibrated σ_DP: 1.7969\n",
            "  Achieved ε: 0.4903 (target: 0.5)\n",
            "  Per-coordinate noise std: 0.0281\n",
            "\n",
            "  Running DP-EGGROLL...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Seed 42: 87.63%\n",
            "    Seed 101: 88.21%\n",
            "    Seed 999: 88.47%\n",
            "  DP-EGGROLL: 88.10 ± 0.35%\n",
            "\n",
            "  Running DP-SGD...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
            "  warnings.warn(\n",
            "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Seed 42: 85.01%\n",
            "    Seed 101: 86.27%\n",
            "    Seed 999: 86.23%\n",
            "  DP-SGD: 85.84 ± 0.58%\n",
            "\n",
            "  Comparison for ε=0.5:\n",
            "    DP-EGGROLL: 88.10% ± 0.35%\n",
            "    DP-SGD:     85.84% ± 0.58%\n",
            "    Gap: +2.27%\n",
            "\n",
            "======================================================================\n",
            "TARGET EPSILON = 1.0\n",
            "======================================================================\n",
            "  Calibrated σ_DP: 1.1621\n",
            "  Achieved ε: 0.9944 (target: 1.0)\n",
            "  Per-coordinate noise std: 0.0182\n",
            "\n",
            "  Running DP-EGGROLL...\n",
            "    Seed 42: 90.15%\n",
            "    Seed 101: 90.40%\n",
            "    Seed 999: 90.09%\n",
            "  DP-EGGROLL: 90.21 ± 0.13%\n",
            "\n",
            "  Running DP-SGD...\n",
            "    Seed 42: 89.53%\n",
            "    Seed 101: 90.01%\n",
            "    Seed 999: 90.54%\n",
            "  DP-SGD: 90.03 ± 0.41%\n",
            "\n",
            "  Comparison for ε=1.0:\n",
            "    DP-EGGROLL: 90.21% ± 0.13%\n",
            "    DP-SGD:     90.03% ± 0.41%\n",
            "    Gap: +0.19%\n",
            "\n",
            "======================================================================\n",
            "TARGET EPSILON = 2.0\n",
            "======================================================================\n",
            "  Calibrated σ_DP: 0.8557\n",
            "  Achieved ε: 1.9967 (target: 2.0)\n",
            "  Per-coordinate noise std: 0.0134\n",
            "\n",
            "  Running DP-EGGROLL...\n",
            "    Seed 42: 91.41%\n",
            "    Seed 101: 91.29%\n",
            "    Seed 999: 91.36%\n",
            "  DP-EGGROLL: 91.35 ± 0.05%\n",
            "\n",
            "  Running DP-SGD...\n",
            "    Seed 42: 91.91%\n",
            "    Seed 101: 92.32%\n",
            "    Seed 999: 92.37%\n",
            "  DP-SGD: 92.20 ± 0.21%\n",
            "\n",
            "  Comparison for ε=2.0:\n",
            "    DP-EGGROLL: 91.35% ± 0.05%\n",
            "    DP-SGD:     92.20% ± 0.21%\n",
            "    Gap: -0.85%\n",
            "\n",
            "======================================================================\n",
            "TARGET EPSILON = 4.0\n",
            "======================================================================\n",
            "  Calibrated σ_DP: 0.6769\n",
            "  Achieved ε: 3.9940 (target: 4.0)\n",
            "  Per-coordinate noise std: 0.0106\n",
            "\n",
            "  Running DP-EGGROLL...\n",
            "    Seed 42: 91.69%\n",
            "    Seed 101: 92.11%\n",
            "    Seed 999: 92.20%\n",
            "  DP-EGGROLL: 92.00 ± 0.22%\n",
            "\n",
            "  Running DP-SGD...\n",
            "    Seed 42: 93.08%\n",
            "    Seed 101: 93.41%\n",
            "    Seed 999: 93.22%\n",
            "  DP-SGD: 93.24 ± 0.14%\n",
            "\n",
            "  Comparison for ε=4.0:\n",
            "    DP-EGGROLL: 92.00% ± 0.22%\n",
            "    DP-SGD:     93.24% ± 0.14%\n",
            "    Gap: -1.24%\n",
            "\n",
            "======================================================================\n",
            "TARGET EPSILON = 8.0\n",
            "======================================================================\n",
            "  Calibrated σ_DP: 0.5496\n",
            "  Achieved ε: 7.9929 (target: 8.0)\n",
            "  Per-coordinate noise std: 0.0086\n",
            "\n",
            "  Running DP-EGGROLL...\n",
            "    Seed 42: 92.68%\n",
            "    Seed 101: 92.36%\n",
            "    Seed 999: 92.51%\n",
            "  DP-EGGROLL: 92.52 ± 0.13%\n",
            "\n",
            "  Running DP-SGD...\n",
            "    Seed 42: 93.93%\n",
            "    Seed 101: 94.09%\n",
            "    Seed 999: 94.03%\n",
            "  DP-SGD: 94.02 ± 0.07%\n",
            "\n",
            "  Comparison for ε=8.0:\n",
            "    DP-EGGROLL: 92.52% ± 0.13%\n",
            "    DP-SGD:     94.02% ± 0.07%\n",
            "    Gap: -1.50%\n",
            "\n",
            "======================================================================\n",
            "LATEX TABLE FOR PAPER (Section 6)\n",
            "======================================================================\n",
            "\n",
            "\\begin{table}[h]\n",
            "\\centering\n",
            "\\caption{MNIST test accuracy (\\%) comparison. Privacy: $(\\varepsilon, 10^{-5})$-DP.\n",
            "Hyperparameters: $N=8192$, $C=4.0$, \n",
            "$m=256$.}\n",
            "\\label{tab:results}\n",
            "\\begin{tabular}{lcc}\n",
            "\\toprule\n",
            "\\textbf{Method} & \\textbf{Test Accuracy} & \\textbf{Privacy} \\\\\n",
            "\\midrule\n",
            "Non-private EGGROLL & 95.8 $\\pm$ 0.2\\% & None \\\\\n",
            "\\midrule\n",
            "DP-EGGROLL & \\textbf{88.1} $\\pm$ 0.4\\% & $\\varepsilon=0.5$ \\\\\n",
            "DP-SGD & 85.8 $\\pm$ 0.6\\% & $\\varepsilon=0.5$ \\\\\n",
            "\\midrule\n",
            "DP-EGGROLL & \\textbf{90.2} $\\pm$ 0.1\\% & $\\varepsilon=1.0$ \\\\\n",
            "DP-SGD & 90.0 $\\pm$ 0.4\\% & $\\varepsilon=1.0$ \\\\\n",
            "\\midrule\n",
            "DP-EGGROLL & 91.4 $\\pm$ 0.0\\% & $\\varepsilon=2.0$ \\\\\n",
            "DP-SGD & \\textbf{92.2} $\\pm$ 0.2\\% & $\\varepsilon=2.0$ \\\\\n",
            "\\midrule\n",
            "DP-EGGROLL & 92.0 $\\pm$ 0.2\\% & $\\varepsilon=4.0$ \\\\\n",
            "DP-SGD & \\textbf{93.2} $\\pm$ 0.1\\% & $\\varepsilon=4.0$ \\\\\n",
            "\\midrule\n",
            "DP-EGGROLL & 92.5 $\\pm$ 0.1\\% & $\\varepsilon=8.0$ \\\\\n",
            "DP-SGD & \\textbf{94.0} $\\pm$ 0.1\\% & $\\varepsilon=8.0$ \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\\end{table}\n",
            "\n",
            "Plot saved to ./results_aligned/\n",
            "\n",
            "======================================================================\n",
            "MATHEMATICAL ALIGNMENT VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "✓ Algorithm 1, Lines 4-6:   Poisson subsampling with q = m/n\n",
            "✓ Algorithm 1, Lines 8-11:  Low-rank perturbations E_i = (1/√r) A_i B_i^T\n",
            "✓ Algorithm 1, Lines 13-15: Per-example loss matrix L[j,i] = ℓ(M_i, x_j)\n",
            "✓ Algorithm 1, Lines 17-20: Vector clipping ℓ̃_j = ℓ_j · min(1, C/‖ℓ_j‖_2)\n",
            "✓ Algorithm 1, Line 22:     Average with NOMINAL batch size: ℓ̄ = (1/m) Σ ℓ̃_j\n",
            "✓ Algorithm 1, Lines 23-24: Gaussian noise Z ~ N(0, σ_DP² C²/m² I_N)\n",
            "                            ℓ̂ = ℓ̄ + Z  (noise added to LOSS)\n",
            "✓ Algorithm 1, Line 27:     Privatized fitness f̃ = -ℓ̂  (THEN negate)\n",
            "✓ Algorithm 1, Line 28:     Reward shaping on f̃ only\n",
            "✓ Algorithm 1, Line 31:     Update μ ← μ + (α/(Nσ)) Σ r_i E_i\n",
            "✓ Lemma 1:                  Sensitivity Δ_2 = C/m\n",
            "✓ Theorem 1:                RDP composition via standard accountant\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT COMPLETE\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"\n",
        "DP-EGGROLL: Differentially Private Evolution Strategies\n",
        "========================================================\n",
        "\n",
        "This script implements the algorithm from:\n",
        "\"Adding Differential Privacy to Evolution Strategies via Fitness Vector Privatization\"\n",
        "\n",
        "Key implementation details:\n",
        "---------------------------------\n",
        "- Noise added to LOSS before negation (Algorithm 1, Lines 23-27)\n",
        "- 1/σ factor included in ES gradient (Equation 4 → absorbed into α)\n",
        "- Nominal batch size m in sensitivity (Lemma 1)\n",
        "- Reward shaping on privatized fitness only (Section 3.6)\n",
        "- Poisson subsampling for privacy amplification (Section 4.2)\n",
        "- Clipping: ℓ̃_j = ℓ_j · min(1, C/‖ℓ_j‖_2) (Equation 5)\n",
        "- Noise: Z ~ N(0, σ_DP² C²/m² · I_N) (Equation 7)\n",
        "\n",
        "PRIVACY MODEL:\n",
        "--------------\n",
        "- Adjacency: ADD/REMOVE (datasets differ by presence/absence of one record)\n",
        "  * Under replace-one adjacency, sensitivity doubles to 2C/m\n",
        "  * Opacus uses add/remove adjacency with sensitivity C/m, matching our implementation\n",
        "- Dataset size n is treated as a fixed public constant (use len(dataset) consistently)\n",
        "  * This is the same assumption Opacus makes (see Opacus GitHub issue #571)\n",
        "  * At large n this is standard practice; at very small n (e.g., 2-3 samples) it can\n",
        "    leak information about dataset membership\n",
        "- Sampling probability q = m/n is data-independent when n is fixed\n",
        "\n",
        "PER-EXAMPLE DECOMPOSABILITY ASSUMPTION:\n",
        "---------------------------------------\n",
        "The sensitivity argument requires per-example losses to be independently\n",
        "clippable. This is VIOLATED by:\n",
        "- BatchNorm in training mode (batch statistics couple examples)\n",
        "- Contrastive losses that pair examples\n",
        "- Any objective with cross-example coupling\n",
        "Use LayerNorm, GroupNorm, or BatchNorm in eval mode instead.\n",
        "\n",
        "CRITICAL RNG REQUIREMENTS (THREE SEPARATE STREAMS):\n",
        "---------------------------------------------------\n",
        "1. Perturbation RNG: PUBLIC (reconstructable from step counter seed)\n",
        "   - Used to generate low-rank perturbations E_i\n",
        "   - Can be deterministic for reproducibility\n",
        "\n",
        "2. DP Noise RNG: SECRET (seeded from OS entropy via secrets.randbits(64))\n",
        "   - Used to generate Gaussian noise Z for privatization\n",
        "   - NEVER log, transmit, or deterministically regenerate\n",
        "\n",
        "3. Subsampling RNG: SECRET if claiming privacy amplification\n",
        "   - Used for Poisson subsampling mask generation\n",
        "   - PoissonSubsampledDataset uses secret_mode=True by default\n",
        "   - If secret_mode=False (for reproducibility), amplification does NOT apply\n",
        "     and accountant should use sample_rate=1.0\n",
        "\n",
        "REPRODUCIBILITY VS DEPLOYMENT:\n",
        "------------------------------\n",
        "This script uses set_seed() for reproducibility in experiments, which affects\n",
        "model initialization and shuffle order but NOT the privacy-critical RNGs:\n",
        "- DP noise uses self._dp_noise_gen (secret, OS entropy)\n",
        "- Subsampling uses secret_mode=True by default (secret, OS entropy)\n",
        "\n",
        "For experiments claiming privacy amplification: seeds can be logged because\n",
        "the privacy-critical RNGs are separate. For maximum security in deployment,\n",
        "also enable Opacus secure_mode for DP-SGD baseline.\n",
        "\n",
        "KEY EQUATIONS (with LaTeX line references):\n",
        "-------------------------------------------\n",
        "1. ES Gradient (Lines 154-157):\n",
        "   ∇_μ J(μ) = (1/σ) E[E · f(μ + σE)]\n",
        "\n",
        "2. EGGROLL Update (Lines 163-165):\n",
        "   μ_{t+1} = μ_t + (α_t/(Nσ)) Σ E_{i,t} · r_i\n",
        "\n",
        "   Note: The 1/σ from the gradient is included explicitly.\n",
        "\n",
        "3. Clipping (Equation 5, Algorithm 1 Line 19-20):\n",
        "   ℓ̃_j = ℓ_j · min(1, C/‖ℓ_j‖_2)\n",
        "\n",
        "4. Scaled Sum with Fixed Denominator (Algorithm 1 Line 22):\n",
        "   ℓ̄ = (1/m) Σ_j ℓ̃_j    [m = NOMINAL batch size, not |B_t|]\n",
        "\n",
        "5. Noise Addition (Algorithm 1 Lines 23-24):\n",
        "   Z ~ N(0, σ_DP² C²/m² · I_N)\n",
        "   ℓ̂ = ℓ̄ + Z\n",
        "\n",
        "6. Privatized Fitness (Algorithm 1 Line 27):\n",
        "   f̃ = -ℓ̂ = -(ℓ̄ + Z)\n",
        "\n",
        "   Note: Negation is post-processing; does not affect privacy.\n",
        "\n",
        "7. Sensitivity (Lemma 1):\n",
        "   Δ_2 = C/m under add/remove adjacency\n",
        "   Δ_2 = 2C/m under replace-one adjacency\n",
        "\n",
        "Author: Anonymous\n",
        "License: MIT\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import secrets  # For cryptographically secure DP noise seeding\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "try:\n",
        "    from opacus.accountants.utils import get_noise_multiplier\n",
        "    from opacus.accountants import RDPAccountant\n",
        "    from opacus import PrivacyEngine\n",
        "    OPACUS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    raise ImportError(\"Opacus required. Install via: pip install opacus\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION (Aligned with LaTeX Section 6.1)\n",
        "# ============================================================================\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RESULTS_DIR = \"./results_aligned\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Random seeds for reproducibility\n",
        "SEEDS = [42, 101, 999]\n",
        "\n",
        "# Privacy parameters (LaTeX Section 6.1)\n",
        "DELTA = 1e-5\n",
        "TARGET_EPSILONS = [0.5, 1.0, 2.0, 4.0, 8.0]  # Include 0.5 for key DP-EGGROLL vs DP-SGD comparison\n",
        "\n",
        "# Training parameters\n",
        "NOMINAL_BATCH_SIZE = 256  # m in the paper (LaTeX Section 6.1)\n",
        "EPOCHS = 10               # Sufficient for convergence\n",
        "\n",
        "# DP-EGGROLL parameters\n",
        "RANK = 1                  # Low-rank parameter r (LaTeX Section 6.1)\n",
        "\n",
        "# Population chunk size for memory management\n",
        "POP_CHUNK_SIZE = 8192\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING WITH POISSON SUBSAMPLING\n",
        "# ============================================================================\n",
        "\n",
        "_DATASET_CACHE = {}\n",
        "\n",
        "class PoissonSubsampledDataset(torch.utils.data.IterableDataset):\n",
        "    \"\"\"\n",
        "    Implements Poisson subsampling per Algorithm 1, Lines 4-6.\n",
        "\n",
        "    Each example is included independently with probability q = m/n.\n",
        "    This is required for privacy amplification by subsampling (Section 4.2).\n",
        "\n",
        "    CRITICAL FOR PRIVACY AMPLIFICATION:\n",
        "    Privacy amplification by subsampling requires that the sampled set remains\n",
        "    SECRET. This implementation uses a dedicated generator seeded from OS entropy\n",
        "    by default (secret_mode=True).\n",
        "\n",
        "    If secret_mode=False (for reproducibility), batch membership becomes\n",
        "    deterministic given the global seed, and privacy amplification does NOT apply.\n",
        "    In that case, the accountant should use sample_rate=1.0 (no amplification).\n",
        "\n",
        "    DATASET SIZE CONVENTION (LaTeX Section 4.1):\n",
        "    Under add/remove adjacency, the dataset size n is treated as a fixed public\n",
        "    constant. This means: use len(dataset) consistently in both the mechanism\n",
        "    and the accountant. This is the same assumption Opacus makes (see GitHub\n",
        "    issue #571). At large n this is standard practice; at very small n it can\n",
        "    leak membership information.\n",
        "\n",
        "    LaTeX Reference (Lines 281-283):\n",
        "    \"Include each x ∈ D independently with probability q = m/n\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, sampling_probability, nominal_batch_size,\n",
        "                 secret_mode=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset: Source dataset\n",
        "            sampling_probability: q = m/n\n",
        "            nominal_batch_size: m (for expected batch count calculation)\n",
        "            secret_mode: If True (default), use OS-entropy-seeded generator for\n",
        "                        subsampling, enabling privacy amplification claims.\n",
        "                        If False, use global RNG (deterministic, NO amplification).\n",
        "        \"\"\"\n",
        "        self.sampling_probability = sampling_probability\n",
        "        self.nominal_batch_size = nominal_batch_size\n",
        "        self.n = len(dataset)\n",
        "        self.secret_mode = secret_mode\n",
        "\n",
        "        # Secret subsampling generator - seeded from OS entropy\n",
        "        # This is CRITICAL for privacy amplification to be valid\n",
        "        if secret_mode:\n",
        "            self._subsampling_gen = torch.Generator()\n",
        "            self._subsampling_gen.manual_seed(secrets.randbits(64))\n",
        "        else:\n",
        "            self._subsampling_gen = None  # Will use global RNG\n",
        "\n",
        "        cache_key = (type(dataset).__name__, self.n)\n",
        "        if cache_key not in _DATASET_CACHE:\n",
        "            all_data = [dataset[i][0] for i in range(self.n)]\n",
        "            all_targets = [dataset[i][1] for i in range(self.n)]\n",
        "            _DATASET_CACHE[cache_key] = (\n",
        "                torch.stack(all_data),\n",
        "                torch.tensor(all_targets)\n",
        "            )\n",
        "        self.all_data, self.all_targets = _DATASET_CACHE[cache_key]\n",
        "\n",
        "    def __iter__(self):\n",
        "        num_batches = int(np.ceil(self.n / self.nominal_batch_size))\n",
        "        for _ in range(num_batches):\n",
        "            # Use secret generator if in secret_mode, else global RNG\n",
        "            if self.secret_mode:\n",
        "                mask = torch.rand(self.n, generator=self._subsampling_gen) < self.sampling_probability\n",
        "            else:\n",
        "                mask = torch.rand(self.n) < self.sampling_probability\n",
        "            indices = torch.where(mask)[0]\n",
        "            if len(indices) == 0:\n",
        "                continue\n",
        "            yield self.all_data[indices], self.all_targets[indices]\n",
        "\n",
        "\n",
        "def get_dataloaders(batch_size, use_poisson=True, secret_subsampling=True):\n",
        "    \"\"\"\n",
        "    Create MNIST data loaders.\n",
        "\n",
        "    Args:\n",
        "        batch_size: Nominal batch size m\n",
        "        use_poisson: If True, use Poisson subsampling for DP-EGGROLL\n",
        "        secret_subsampling: If True (default), subsampling uses OS-entropy-seeded\n",
        "                          generator, enabling privacy amplification claims.\n",
        "                          If False, uses global RNG (for reproducibility only;\n",
        "                          privacy amplification does NOT apply).\n",
        "\n",
        "    IMPORTANT: For valid privacy amplification, secret_subsampling must be True\n",
        "    and the subsampling generator must never be logged or reconstructed.\n",
        "\n",
        "    The sample_rate q = m/n uses n = len(train_dataset). This n is treated as a\n",
        "    fixed public constant, the same assumption Opacus makes. At large n this is\n",
        "    standard practice; at very small n it can leak membership information.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
        "\n",
        "    train_len = len(train_dataset)\n",
        "    sample_rate = batch_size / train_len\n",
        "\n",
        "    if use_poisson:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            PoissonSubsampledDataset(train_dataset, sample_rate, batch_size,\n",
        "                                    secret_mode=secret_subsampling),\n",
        "            batch_size=None,\n",
        "        )\n",
        "    else:\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=1000, shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, train_len, sample_rate\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL (LaTeX Section 6.1: \"784-256-10, approximately 200K parameters\")\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    \"\"\"Two-layer MLP for MNIST (784 -> 256 -> 10, no biases).\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(784, 256, bias=False)\n",
        "        self.fc2 = nn.Linear(256, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DP-EGGROLL OPTIMIZER\n",
        "# ============================================================================\n",
        "\n",
        "class DPEggrollOptimizer:\n",
        "    \"\"\"\n",
        "    DP-EGGROLL optimizer implementing Algorithm 1 from the LaTeX paper.\n",
        "\n",
        "    CRITICAL ALIGNMENT NOTES:\n",
        "    -------------------------\n",
        "\n",
        "    1. NOISE ORDER (Algorithm 1, Lines 23-27):\n",
        "       The paper specifies:\n",
        "         Line 24: ℓ̂ ← ℓ̄ + Z         (add noise to LOSS)\n",
        "         Line 27: f̃ ← -ℓ̂            (THEN negate)\n",
        "\n",
        "       So: f̃ = -(ℓ̄ + Z) = -ℓ̄ - Z\n",
        "\n",
        "       This is mathematically equivalent to -ℓ̄ + Z (since Z is symmetric),\n",
        "       but we follow the paper's exact order for clarity.\n",
        "\n",
        "    2. ES GRADIENT AND UPDATE:\n",
        "       LaTeX Equation (line 154-157): ∇J = (1/σ) E[E·f]\n",
        "       LaTeX Update (line 163-165):   μ_{t+1} = μ_t + (α/N) Σ E_i · f_i\n",
        "\n",
        "       The 1/σ is absorbed into α. Equivalently, we can write:\n",
        "         μ_{t+1} = μ_t + (α/(Nσ)) Σ E_i · r_i\n",
        "\n",
        "       We use the explicit 1/σ form for mathematical clarity.\n",
        "\n",
        "    3. SENSITIVITY (Lemma 1, Lines 358-370):\n",
        "       Δ_2 = C/m where m is the NOMINAL batch size.\n",
        "       We divide by m even when |B_t| < m (zero-padding convention).\n",
        "\n",
        "    4. ADJACENCY CONVENTION (LaTeX Lines 480-485):\n",
        "       We use ADD/REMOVE adjacency: neighboring datasets differ by one record\n",
        "       being present or absent. Under REPLACE-ONE adjacency, sensitivity doubles\n",
        "       to 2C/m. Opacus uses add/remove adjacency with sensitivity C/m, matching\n",
        "       our implementation exactly.\n",
        "\n",
        "    5. RNG STREAMS (CRITICAL FOR PRIVACY):\n",
        "       - Perturbation RNG: PUBLIC, seeded deterministically, can be reconstructed\n",
        "       - DP Noise RNG: SECRET, seeded from OS entropy, never logged/reconstructable\n",
        "\n",
        "       These MUST be separate streams. Using global seeded RNG for DP noise\n",
        "       would make the mechanism non-private.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, pop_size, sigma_es, lr, loss_clip,\n",
        "                 noise_multiplier, nominal_batch_size, rank=1,\n",
        "                 dp_mode=True, device=\"cpu\", pop_chunk_size=4000,\n",
        "                 secret_noise_seed=None, use_centered_clipping=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: Neural network to optimize\n",
        "            pop_size: Population size N (LaTeX: Lines 274, 592)\n",
        "            sigma_es: ES perturbation scale σ (LaTeX: Line 275)\n",
        "            lr: Learning rate α (LaTeX: Line 276)\n",
        "            loss_clip: Clipping constant C (LaTeX: Lines 276, 596)\n",
        "            noise_multiplier: DP noise multiplier σ_DP (LaTeX: Line 277, 597)\n",
        "            nominal_batch_size: m for sensitivity calculation (LaTeX: Lemma 1)\n",
        "            rank: Low-rank parameter r (LaTeX: Line 275)\n",
        "            dp_mode: If True, apply DP mechanism\n",
        "            device: Computation device\n",
        "            pop_chunk_size: Max population per chunk for memory\n",
        "            secret_noise_seed: If None (default), use OS entropy for DP noise.\n",
        "                              If set, use deterministic seed (FOR DEBUGGING ONLY,\n",
        "                              NOT A VALID DP DEPLOYMENT).\n",
        "            use_centered_clipping: If True, center each per-example vector across\n",
        "                              candidates before clipping. This removes the common-mode\n",
        "                              component that dominates ||ℓ_j||_2, allowing smaller C\n",
        "                              without heavy clipping. The centering projection is\n",
        "                              non-expansive, so sensitivity bound C/m still holds.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.pop_size = pop_size\n",
        "        self.sigma_es = sigma_es\n",
        "        self.lr = lr\n",
        "        self.loss_clip = loss_clip\n",
        "        self.noise_multiplier = noise_multiplier\n",
        "        self.nominal_batch_size = nominal_batch_size\n",
        "        self.rank = rank\n",
        "        self.dp_mode = dp_mode\n",
        "        self.device = device\n",
        "        self.pop_chunk_size = pop_chunk_size\n",
        "        self.step_counter = 0\n",
        "        self.use_centered_clipping = use_centered_clipping\n",
        "\n",
        "        # Diagnostic counters (for debugging, do NOT expose in production)\n",
        "        self._clip_fractions = []  # Track what fraction of examples are clipped\n",
        "\n",
        "        # =================================================================\n",
        "        # CRITICAL: Separate RNG streams for privacy\n",
        "        # =================================================================\n",
        "        # Perturbation RNG: PUBLIC (seeded deterministically per step)\n",
        "        # - Created fresh each step using step_counter as seed\n",
        "        # - Adversary can reconstruct perturbations (this is fine)\n",
        "\n",
        "        # DP Noise RNG: SECRET (seeded from OS entropy)\n",
        "        # - Must NEVER be logged, transmitted, or reconstructable\n",
        "        # - If secret_noise_seed is set, we're in debug mode (NOT private)\n",
        "        self._dp_noise_gen = torch.Generator(device=device)\n",
        "        if secret_noise_seed is not None:\n",
        "            print(\"Warning: using deterministic DP noise seed (debug mode only)\")\n",
        "            self._dp_noise_gen.manual_seed(secret_noise_seed)\n",
        "        else:\n",
        "            # Seed from OS entropy - this is the secure default\n",
        "            self._dp_noise_gen.manual_seed(secrets.randbits(64))\n",
        "\n",
        "    def get_clip_diagnostics(self):\n",
        "        \"\"\"\n",
        "        Return clipping diagnostics from training.\n",
        "\n",
        "        WARNING: These diagnostics are for debugging/development only.\n",
        "        In a real DP deployment, exposing per-step clipping fractions\n",
        "        could leak information about the data. Only use for:\n",
        "        - Hyperparameter tuning on public/synthetic data\n",
        "        - Understanding the clipping regime\n",
        "        - Validating your choice of C\n",
        "\n",
        "        Returns:\n",
        "            dict with:\n",
        "                - mean_clip_fraction: average fraction of examples clipped\n",
        "                - median_clip_fraction: median fraction\n",
        "                - num_steps: number of steps recorded\n",
        "        \"\"\"\n",
        "        if not self._clip_fractions:\n",
        "            return {\"mean_clip_fraction\": None, \"median_clip_fraction\": None, \"num_steps\": 0}\n",
        "\n",
        "        fracs = np.array(self._clip_fractions)\n",
        "        return {\n",
        "            \"mean_clip_fraction\": float(fracs.mean()),\n",
        "            \"median_clip_fraction\": float(np.median(fracs)),\n",
        "            \"num_steps\": len(fracs)\n",
        "        }\n",
        "\n",
        "    def reset_diagnostics(self):\n",
        "        \"\"\"Clear diagnostic counters.\"\"\"\n",
        "        self._clip_fractions = []\n",
        "\n",
        "    def step(self, data, target):\n",
        "        \"\"\"\n",
        "        One DP-EGGROLL optimization step (Algorithm 1).\n",
        "\n",
        "        Returns: mean fitness (for logging)\n",
        "        \"\"\"\n",
        "        if self.pop_size > self.pop_chunk_size:\n",
        "            return self._step_chunked(data, target)\n",
        "        else:\n",
        "            return self._step_vectorized(data, target)\n",
        "\n",
        "    def _step_vectorized(self, data, target):\n",
        "        \"\"\"\n",
        "        Vectorized implementation following Algorithm 1 exactly.\n",
        "\n",
        "        Line-by-line correspondence with LaTeX Algorithm 1:\n",
        "        - Lines 4-6: Poisson subsampling (handled by data loader)\n",
        "        - Lines 8-11: Generate perturbations\n",
        "        - Lines 13-15: Compute per-example loss matrix\n",
        "        - Lines 17-20: Clip per-example vectors\n",
        "        - Lines 22-24: Average and add noise\n",
        "        - Line 27: Form privatized fitness\n",
        "        - Line 28: Shape rewards\n",
        "        - Line 31: EGGROLL update\n",
        "        \"\"\"\n",
        "        actual_batch_size = data.size(0)\n",
        "        step_seed = self.step_counter * 1000000\n",
        "        self.step_counter += 1\n",
        "\n",
        "        # Get base parameters\n",
        "        params = list(self.model.parameters())\n",
        "        W1, W2 = params[0], params[1]  # (256, 784), (10, 256)\n",
        "\n",
        "        # =====================================================================\n",
        "        # Algorithm 1, Lines 8-11: Generate perturbations (public randomness)\n",
        "        # E_{i,t} = (1/√r) A_{i,t} B_{i,t}^T\n",
        "        # =====================================================================\n",
        "        rng = torch.Generator(device=self.device)\n",
        "        rng.manual_seed(step_seed)\n",
        "\n",
        "        scale = 1.0 / math.sqrt(self.rank)\n",
        "\n",
        "        # Layer 1 perturbations\n",
        "        A1 = torch.randn(self.pop_size, 256, self.rank, device=self.device, generator=rng)\n",
        "        B1 = torch.randn(self.pop_size, 784, self.rank, device=self.device, generator=rng)\n",
        "        E1 = scale * torch.bmm(A1, B1.transpose(1, 2))  # (N, 256, 784)\n",
        "\n",
        "        # Layer 2 perturbations\n",
        "        A2 = torch.randn(self.pop_size, 10, self.rank, device=self.device, generator=rng)\n",
        "        B2 = torch.randn(self.pop_size, 256, self.rank, device=self.device, generator=rng)\n",
        "        E2 = scale * torch.bmm(A2, B2.transpose(1, 2))  # (N, 10, 256)\n",
        "\n",
        "        # Form perturbed weights: M_{i,t} = μ_t + σ E_{i,t} (Line 11)\n",
        "        W1_all = W1.unsqueeze(0) + self.sigma_es * E1\n",
        "        W2_all = W2.unsqueeze(0) + self.sigma_es * E2\n",
        "\n",
        "        # Flatten input\n",
        "        x = data.view(actual_batch_size, -1)  # (B, 784)\n",
        "\n",
        "        # =====================================================================\n",
        "        # Algorithm 1, Lines 13-15: Compute per-example loss matrix\n",
        "        # L[j,i] = ℓ(M_{i,t}, x_j) for all j ∈ B_t, i ∈ [N]\n",
        "        # =====================================================================\n",
        "        with torch.no_grad():\n",
        "            hidden = torch.einsum('bd,nod->nbo', x, W1_all)  # (N, B, 256)\n",
        "            hidden = F.relu(hidden)\n",
        "            logits = torch.einsum('nbd,nod->nbo', hidden, W2_all)  # (N, B, 10)\n",
        "\n",
        "            # Per-example losses: L[j,i] = ℓ(M_i, x_j)\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            target_exp = target.unsqueeze(0).expand(self.pop_size, -1)\n",
        "            losses = F.nll_loss(\n",
        "                log_probs.reshape(-1, 10),\n",
        "                target_exp.reshape(-1),\n",
        "                reduction='none'\n",
        "            ).view(self.pop_size, actual_batch_size)  # (N, B)\n",
        "\n",
        "        # =====================================================================\n",
        "        # DP Mechanism (Algorithm 1, Lines 17-28)\n",
        "        # =====================================================================\n",
        "        if self.dp_mode:\n",
        "            # -----------------------------------------------------------------\n",
        "            # Lines 17-20: Clip per-example vectors\n",
        "            # ℓ_j = L[j, :]  (per-example loss vector across candidates)\n",
        "            #\n",
        "            # OPTIONAL: Per-example centering before clipping\n",
        "            # If use_centered_clipping=True, we first subtract the per-example\n",
        "            # mean across candidates: u_j = ℓ_j - mean(ℓ_j)\n",
        "            # This removes the common-mode component that dominates ||ℓ_j||_2.\n",
        "            # The centering projection P = I - (1/N)11^T is non-expansive\n",
        "            # (||Px||_2 <= ||x||_2), so clipping to C still gives sensitivity C/m.\n",
        "            # -----------------------------------------------------------------\n",
        "            per_example_losses = losses.t()  # (|B_t|, N) - each row is ℓ_j\n",
        "\n",
        "            if self.use_centered_clipping:\n",
        "                # Center each row: u_j = ℓ_j - mean(ℓ_j)\n",
        "                row_means = per_example_losses.mean(dim=1, keepdim=True)\n",
        "                vectors_to_clip = per_example_losses - row_means\n",
        "            else:\n",
        "                vectors_to_clip = per_example_losses\n",
        "\n",
        "            # Compute norms and clip factors\n",
        "            norms = vectors_to_clip.norm(dim=1, keepdim=True)  # ||ℓ_j||_2 or ||u_j||_2\n",
        "            clip_factors = torch.clamp(self.loss_clip / (norms + 1e-8), max=1.0)\n",
        "\n",
        "            # Track clipping diagnostics (DO NOT LOG IN PRODUCTION)\n",
        "            frac_clipped = (clip_factors < 1.0).float().mean().item()\n",
        "            self._clip_fractions.append(frac_clipped)\n",
        "\n",
        "            # Apply clipping: ℓ̃_j = ℓ_j · min(1, C/||ℓ_j||_2)\n",
        "            clipped_vectors = vectors_to_clip * clip_factors\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # Line 22: Average with NOMINAL batch size\n",
        "            # ℓ̄ = (1/m) Σ_j ℓ̃_j\n",
        "            # CRITICAL: Use m (nominal), not |B_t| (actual)\n",
        "            # -----------------------------------------------------------------\n",
        "            avg_clipped = clipped_vectors.sum(dim=0) / self.nominal_batch_size  # (N,)\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # Lines 23-24: Add Gaussian noise\n",
        "            # Z ~ N(0, σ_DP² C²/m² · I_N)\n",
        "            # ℓ̂ = ℓ̄ + Z\n",
        "            # CRITICAL: Use SECRET RNG (self._dp_noise_gen), not global RNG\n",
        "            # -----------------------------------------------------------------\n",
        "            sensitivity = self.loss_clip / self.nominal_batch_size  # Δ_2 = C/m\n",
        "            noise_std = self.noise_multiplier * sensitivity\n",
        "            noise = torch.randn(\n",
        "                self.pop_size,\n",
        "                device=self.device,\n",
        "                generator=self._dp_noise_gen  # SECRET RNG\n",
        "            ) * noise_std\n",
        "            noisy_avg = avg_clipped + noise  # ℓ̂ = ℓ̄ + Z\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # Line 27: Form privatized fitness\n",
        "            # f̃ = -ℓ̂\n",
        "            # CRITICAL: Negate AFTER adding noise (per paper specification)\n",
        "            # -----------------------------------------------------------------\n",
        "            fitness = -noisy_avg  # f̃ = -ℓ̂ = -(ℓ̄ + Z)\n",
        "\n",
        "            # -----------------------------------------------------------------\n",
        "            # Line 28: Reward shaping on PRIVATIZED fitness only\n",
        "            # r = shape(f̃) using only statistics of f̃\n",
        "            # -----------------------------------------------------------------\n",
        "            rewards = (fitness - fitness.mean()) / (fitness.std() + 1e-8)\n",
        "        else:\n",
        "            # Non-private: standard batch-averaged fitness\n",
        "            fitness = -losses.mean(dim=1)\n",
        "            rewards = (fitness - fitness.mean()) / (fitness.std() + 1e-8)\n",
        "\n",
        "        # =====================================================================\n",
        "        # Algorithm 1, Line 31: EGGROLL update\n",
        "        # μ_{t+1} = μ_t + (α_t/N) Σ E_{i,t} · r_i\n",
        "        #\n",
        "        # NOTE ON 1/σ FACTOR:\n",
        "        # The ES gradient (Equation 4) includes 1/σ: ∇J = (1/σ) E[E·f]\n",
        "        # The EGGROLL update (Equation 5) absorbs this into α.\n",
        "        #\n",
        "        # For mathematical clarity, we use the explicit form:\n",
        "        #   μ_{t+1} = μ_t + (α/(Nσ)) Σ E_i · r_i\n",
        "        # =====================================================================\n",
        "        with torch.no_grad():\n",
        "            # Gradient for layer 1: g1 = (1/(Nσ)) Σ r_i E1_i\n",
        "            grad1 = torch.einsum('n,nij->ij', rewards, E1) / (self.pop_size * self.sigma_es)\n",
        "            W1.add_(grad1, alpha=self.lr)\n",
        "\n",
        "            # Gradient for layer 2: g2 = (1/(Nσ)) Σ r_i E2_i\n",
        "            grad2 = torch.einsum('n,nij->ij', rewards, E2) / (self.pop_size * self.sigma_es)\n",
        "            W2.add_(grad2, alpha=self.lr)\n",
        "\n",
        "        return fitness.mean().item()\n",
        "\n",
        "    def _step_chunked(self, data, target):\n",
        "        \"\"\"Chunked implementation for large populations.\"\"\"\n",
        "        actual_batch_size = data.size(0)\n",
        "        chunk_size = self.pop_chunk_size\n",
        "        num_chunks = math.ceil(self.pop_size / chunk_size)\n",
        "\n",
        "        step_seed = self.step_counter * 1000000\n",
        "        self.step_counter += 1\n",
        "\n",
        "        params = list(self.model.parameters())\n",
        "        W1, W2 = params[0], params[1]\n",
        "        x = data.view(actual_batch_size, -1)\n",
        "\n",
        "        # Phase 1: Compute all losses\n",
        "        all_losses = []\n",
        "        rng = torch.Generator(device=self.device)\n",
        "\n",
        "        for chunk_idx in range(num_chunks):\n",
        "            start = chunk_idx * chunk_size\n",
        "            end = min(start + chunk_size, self.pop_size)\n",
        "            curr_size = end - start\n",
        "\n",
        "            rng.manual_seed(step_seed + chunk_idx)\n",
        "            scale = 1.0 / math.sqrt(self.rank)\n",
        "\n",
        "            A1 = torch.randn(curr_size, 256, self.rank, device=self.device, generator=rng)\n",
        "            B1 = torch.randn(curr_size, 784, self.rank, device=self.device, generator=rng)\n",
        "            E1 = scale * torch.bmm(A1, B1.transpose(1, 2))\n",
        "\n",
        "            A2 = torch.randn(curr_size, 10, self.rank, device=self.device, generator=rng)\n",
        "            B2 = torch.randn(curr_size, 256, self.rank, device=self.device, generator=rng)\n",
        "            E2 = scale * torch.bmm(A2, B2.transpose(1, 2))\n",
        "\n",
        "            W1_chunk = W1.unsqueeze(0) + self.sigma_es * E1\n",
        "            W2_chunk = W2.unsqueeze(0) + self.sigma_es * E2\n",
        "\n",
        "            with torch.no_grad():\n",
        "                hidden = torch.einsum('bd,nod->nbo', x, W1_chunk)\n",
        "                hidden = F.relu(hidden)\n",
        "                logits = torch.einsum('nbd,nod->nbo', hidden, W2_chunk)\n",
        "\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "                target_exp = target.unsqueeze(0).expand(curr_size, -1)\n",
        "                losses = F.nll_loss(\n",
        "                    log_probs.reshape(-1, 10),\n",
        "                    target_exp.reshape(-1),\n",
        "                    reduction='none'\n",
        "                ).view(curr_size, actual_batch_size)\n",
        "                all_losses.append(losses)\n",
        "\n",
        "            del E1, E2, W1_chunk, W2_chunk, A1, B1, A2, B2\n",
        "\n",
        "        all_losses = torch.cat(all_losses, dim=0)  # (N, B)\n",
        "\n",
        "        # Phase 2: DP mechanism (exactly as in _step_vectorized)\n",
        "        if self.dp_mode:\n",
        "            per_example_losses = all_losses.t()  # (|B_t|, N)\n",
        "\n",
        "            # Optional centering before clipping\n",
        "            if self.use_centered_clipping:\n",
        "                row_means = per_example_losses.mean(dim=1, keepdim=True)\n",
        "                vectors_to_clip = per_example_losses - row_means\n",
        "            else:\n",
        "                vectors_to_clip = per_example_losses\n",
        "\n",
        "            norms = vectors_to_clip.norm(dim=1, keepdim=True)\n",
        "            clip_factors = torch.clamp(self.loss_clip / (norms + 1e-8), max=1.0)\n",
        "\n",
        "            # Track clipping diagnostics\n",
        "            frac_clipped = (clip_factors < 1.0).float().mean().item()\n",
        "            self._clip_fractions.append(frac_clipped)\n",
        "\n",
        "            clipped_vectors = vectors_to_clip * clip_factors\n",
        "\n",
        "            # Average with NOMINAL batch size\n",
        "            avg_clipped = clipped_vectors.sum(dim=0) / self.nominal_batch_size\n",
        "\n",
        "            # Add noise to LOSS using SECRET RNG\n",
        "            sensitivity = self.loss_clip / self.nominal_batch_size\n",
        "            noise_std = self.noise_multiplier * sensitivity\n",
        "            noise = torch.randn(\n",
        "                self.pop_size,\n",
        "                device=self.device,\n",
        "                generator=self._dp_noise_gen  # SECRET RNG\n",
        "            ) * noise_std\n",
        "            noisy_avg = avg_clipped + noise\n",
        "\n",
        "            # THEN negate to get fitness\n",
        "            fitness = -noisy_avg\n",
        "            rewards = (fitness - fitness.mean()) / (fitness.std() + 1e-8)\n",
        "        else:\n",
        "            fitness = -all_losses.mean(dim=1)\n",
        "            rewards = (fitness - fitness.mean()) / (fitness.std() + 1e-8)\n",
        "\n",
        "        del all_losses\n",
        "\n",
        "        # Phase 3: Compute gradients\n",
        "        grad1_acc = torch.zeros_like(W1)\n",
        "        grad2_acc = torch.zeros_like(W2)\n",
        "        rng = torch.Generator(device=self.device)\n",
        "\n",
        "        for chunk_idx in range(num_chunks):\n",
        "            start = chunk_idx * chunk_size\n",
        "            end = min(start + chunk_size, self.pop_size)\n",
        "            curr_size = end - start\n",
        "\n",
        "            rng.manual_seed(step_seed + chunk_idx)\n",
        "            scale = 1.0 / math.sqrt(self.rank)\n",
        "\n",
        "            A1 = torch.randn(curr_size, 256, self.rank, device=self.device, generator=rng)\n",
        "            B1 = torch.randn(curr_size, 784, self.rank, device=self.device, generator=rng)\n",
        "            E1 = scale * torch.bmm(A1, B1.transpose(1, 2))\n",
        "\n",
        "            A2 = torch.randn(curr_size, 10, self.rank, device=self.device, generator=rng)\n",
        "            B2 = torch.randn(curr_size, 256, self.rank, device=self.device, generator=rng)\n",
        "            E2 = scale * torch.bmm(A2, B2.transpose(1, 2))\n",
        "\n",
        "            chunk_rewards = rewards[start:end]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                grad1_acc += torch.einsum('n,nij->ij', chunk_rewards, E1)\n",
        "                grad2_acc += torch.einsum('n,nij->ij', chunk_rewards, E2)\n",
        "\n",
        "            del E1, E2, A1, B1, A2, B2\n",
        "\n",
        "        with torch.no_grad():\n",
        "            W1.add_(grad1_acc / (self.pop_size * self.sigma_es), alpha=self.lr)\n",
        "            W2.add_(grad2_acc / (self.pop_size * self.sigma_es), alpha=self.lr)\n",
        "\n",
        "        return fitness.mean().item()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PRIVACY ACCOUNTING\n",
        "# ============================================================================\n",
        "\n",
        "def calibrate_noise_multiplier(target_epsilon, steps, sample_rate, delta=DELTA):\n",
        "    \"\"\"\n",
        "    Calibrate σ_DP for target (ε,δ)-DP using RDP accountant.\n",
        "\n",
        "    Per Theorem 1 (LaTeX Lines 403-415): Uses subsampled Gaussian mechanism.\n",
        "\n",
        "    Args:\n",
        "        target_epsilon: Target ε for (ε,δ)-DP\n",
        "        steps: ACTUAL number of training steps (not estimated from epochs/sample_rate)\n",
        "        sample_rate: Sampling probability q = m/n\n",
        "        delta: Target δ\n",
        "    \"\"\"\n",
        "    if target_epsilon == float('inf'):\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        noise_mult = get_noise_multiplier(\n",
        "            target_epsilon=target_epsilon,\n",
        "            target_delta=delta,\n",
        "            sample_rate=sample_rate,\n",
        "            steps=steps,\n",
        "            accountant=\"rdp\"\n",
        "        )\n",
        "        return noise_mult\n",
        "    except Exception as e:\n",
        "        print(f\"  Warning: Could not calibrate for ε={target_epsilon}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def compute_achieved_epsilon(noise_multiplier, steps, sample_rate, delta=DELTA):\n",
        "    \"\"\"Compute the actual (ε, δ) achieved given noise multiplier.\"\"\"\n",
        "    accountant = RDPAccountant()\n",
        "    for _ in range(steps):\n",
        "        accountant.step(noise_multiplier=noise_multiplier, sample_rate=sample_rate)\n",
        "\n",
        "    return accountant.get_epsilon(delta=delta)\n",
        "\n",
        "\n",
        "def count_steps_per_epoch(train_len, nominal_batch_size):\n",
        "    \"\"\"\n",
        "    Count actual steps per epoch for Poisson subsampling.\n",
        "\n",
        "    Under Poisson subsampling, we iterate ceil(n/m) times per epoch\n",
        "    to match standard training dynamics.\n",
        "    \"\"\"\n",
        "    return int(np.ceil(train_len / nominal_batch_size))\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    \"\"\"Evaluate model accuracy on test set.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "\n",
        "def train_dp_eggroll(params, epochs, sample_rate, dp_mode=True, verbose=False):\n",
        "    \"\"\"\n",
        "    Train with DP-EGGROLL.\n",
        "\n",
        "    Args:\n",
        "        params: dict with keys: pop_size, sigma_es, lr, loss_clip, noise_multiplier\n",
        "        epochs: number of epochs\n",
        "        sample_rate: q = m/n\n",
        "        dp_mode: whether to apply DP\n",
        "        verbose: print progress\n",
        "\n",
        "    Returns:\n",
        "        final test accuracy\n",
        "    \"\"\"\n",
        "    train_loader, test_loader, _, _ = get_dataloaders(NOMINAL_BATCH_SIZE, use_poisson=True)\n",
        "    model = SimpleMLP().to(DEVICE)\n",
        "\n",
        "    optimizer = DPEggrollOptimizer(\n",
        "        model=model,\n",
        "        pop_size=params['pop_size'],\n",
        "        sigma_es=params['sigma_es'],\n",
        "        lr=params['lr'],\n",
        "        loss_clip=params['loss_clip'],\n",
        "        noise_multiplier=params.get('noise_multiplier', 0.0),\n",
        "        nominal_batch_size=NOMINAL_BATCH_SIZE,\n",
        "        rank=RANK,\n",
        "        dp_mode=dp_mode,\n",
        "        device=DEVICE,\n",
        "        pop_chunk_size=POP_CHUNK_SIZE\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            optimizer.step(data, target)\n",
        "\n",
        "        if verbose:\n",
        "            acc = evaluate(model, test_loader, DEVICE)\n",
        "            print(f\"    Epoch {epoch+1}/{epochs}: {acc:.2f}%\")\n",
        "\n",
        "    return evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "\n",
        "def train_dp_sgd(noise_multiplier, epochs, max_grad_norm=1.0):\n",
        "    \"\"\"\n",
        "    Train with DP-SGD using Opacus.\n",
        "\n",
        "    IMPORTANT: For fair comparison with DP-EGGROLL, we use Poisson subsampling\n",
        "    via Opacus's DPDataLoader. We pass a standard DataLoader and let Opacus\n",
        "    wrap it with poisson_sampling=True, which matches the subsampling model\n",
        "    used in our RDP accounting.\n",
        "\n",
        "    Adjacency convention: Opacus uses add/remove adjacency with sensitivity C/m,\n",
        "    matching our DP-EGGROLL implementation exactly. Both methods use the same\n",
        "    privacy accounting assumptions.\n",
        "    \"\"\"\n",
        "    # Use standard DataLoader - Opacus will wrap it with Poisson sampling\n",
        "    train_loader, test_loader, train_len, sample_rate = get_dataloaders(\n",
        "        NOMINAL_BATCH_SIZE, use_poisson=False\n",
        "    )\n",
        "    model = SimpleMLP().to(DEVICE)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
        "    model, optimizer, train_loader = privacy_engine.make_private(\n",
        "        module=model,\n",
        "        optimizer=optimizer,\n",
        "        data_loader=train_loader,\n",
        "        noise_multiplier=noise_multiplier,\n",
        "        max_grad_norm=max_grad_norm,\n",
        "        poisson_sampling=True,  # Opacus wraps the DataLoader with Poisson sampling\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    return evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXPERIMENT\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"DP-EGGROLL: Experimental Validation (Aligned with LaTeX)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Device: {DEVICE}\")\n",
        "    print(f\"  Nominal batch size m: {NOMINAL_BATCH_SIZE}\")\n",
        "    print(f\"  Target δ: {DELTA}\")\n",
        "    print(f\"  Target epsilons: {TARGET_EPSILONS}\")\n",
        "    print(f\"  Epochs: {EPOCHS}\")\n",
        "    print(f\"  Seeds: {SEEDS}\")\n",
        "\n",
        "    # Get sample rate and compute ACTUAL step counts\n",
        "    _, _, train_len, sample_rate = get_dataloaders(NOMINAL_BATCH_SIZE, use_poisson=False)\n",
        "    steps_per_epoch = count_steps_per_epoch(train_len, NOMINAL_BATCH_SIZE)\n",
        "    total_steps = EPOCHS * steps_per_epoch\n",
        "    print(f\"  Dataset size n: {train_len}\")\n",
        "    print(f\"  Sample rate q = m/n: {sample_rate:.6f}\")\n",
        "    print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
        "    print(f\"  Total steps T: {total_steps}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # =========================================================================\n",
        "    # Hyperparameters (tuned for DP setting per Section 5.5)\n",
        "    # =========================================================================\n",
        "    # Key insight from Section 5.1: DP noise variance = O(σ_DP² C²/(m² N σ²))\n",
        "    # Larger N helps average out DP noise\n",
        "\n",
        "    BASE_PARAMS = {\n",
        "        'pop_size': 8192,      # Moderate N (paper recommends ≲10³ but larger helps)\n",
        "        'sigma_es': 0.03,      # ES perturbation scale σ\n",
        "        'lr': 0.02,            # Learning rate α (absorbs 1/σ scaling)\n",
        "        'loss_clip': 4.0,      # Clipping norm C\n",
        "    }\n",
        "\n",
        "    print(f\"\\n  Hyperparameters:\")\n",
        "    print(f\"    Population size N: {BASE_PARAMS['pop_size']}\")\n",
        "    print(f\"    ES noise σ: {BASE_PARAMS['sigma_es']}\")\n",
        "    print(f\"    Learning rate α: {BASE_PARAMS['lr']}\")\n",
        "    print(f\"    Clipping norm C: {BASE_PARAMS['loss_clip']}\")\n",
        "\n",
        "    sensitivity = BASE_PARAMS['loss_clip'] / NOMINAL_BATCH_SIZE\n",
        "    print(f\"    Sensitivity Δ_2 = C/m: {sensitivity:.6f}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Non-Private Baseline\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"NON-PRIVATE EGGROLL BASELINE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    np_accs = []\n",
        "    for seed in SEEDS:\n",
        "        set_seed(seed)\n",
        "        acc = train_dp_eggroll(BASE_PARAMS, EPOCHS, sample_rate, dp_mode=False)\n",
        "        np_accs.append(acc)\n",
        "        print(f\"  Seed {seed}: {acc:.2f}%\")\n",
        "\n",
        "    np_mean, np_std = np.mean(np_accs), np.std(np_accs)\n",
        "    print(f\"\\n  Non-Private EGGROLL: {np_mean:.2f} ± {np_std:.2f}%\")\n",
        "\n",
        "    results.append({\n",
        "        'epsilon': float('inf'),\n",
        "        'method': 'EGGROLL (non-private)',\n",
        "        'mean_acc': np_mean,\n",
        "        'std_acc': np_std,\n",
        "        'noise_multiplier': 0.0\n",
        "    })\n",
        "\n",
        "    # =========================================================================\n",
        "    # DP-EGGROLL and DP-SGD for each epsilon\n",
        "    # =========================================================================\n",
        "    for eps in TARGET_EPSILONS:\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"TARGET EPSILON = {eps}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Calibrate noise using ACTUAL step count\n",
        "        noise_mult = calibrate_noise_multiplier(eps, total_steps, sample_rate)\n",
        "        if noise_mult is None:\n",
        "            print(f\"  Skipping ε={eps} (calibration failed)\")\n",
        "            continue\n",
        "\n",
        "        # Verify achieved epsilon using ACTUAL step count\n",
        "        achieved_eps = compute_achieved_epsilon(noise_mult, total_steps, sample_rate)\n",
        "        print(f\"  Calibrated σ_DP: {noise_mult:.4f}\")\n",
        "        print(f\"  Achieved ε: {achieved_eps:.4f} (target: {eps})\")\n",
        "\n",
        "        # Per-coordinate noise analysis\n",
        "        noise_per_coord = noise_mult * sensitivity\n",
        "        print(f\"  Per-coordinate noise std: {noise_per_coord:.4f}\")\n",
        "\n",
        "        # DP-EGGROLL\n",
        "        print(f\"\\n  Running DP-EGGROLL...\")\n",
        "        params = BASE_PARAMS.copy()\n",
        "        params['noise_multiplier'] = noise_mult\n",
        "\n",
        "        egg_accs = []\n",
        "        for seed in SEEDS:\n",
        "            set_seed(seed)\n",
        "            acc = train_dp_eggroll(params, EPOCHS, sample_rate, dp_mode=True)\n",
        "            egg_accs.append(acc)\n",
        "            print(f\"    Seed {seed}: {acc:.2f}%\")\n",
        "\n",
        "        egg_mean, egg_std = np.mean(egg_accs), np.std(egg_accs)\n",
        "        print(f\"  DP-EGGROLL: {egg_mean:.2f} ± {egg_std:.2f}%\")\n",
        "\n",
        "        results.append({\n",
        "            'epsilon': eps,\n",
        "            'method': 'DP-EGGROLL',\n",
        "            'mean_acc': egg_mean,\n",
        "            'std_acc': egg_std,\n",
        "            'noise_multiplier': noise_mult\n",
        "        })\n",
        "\n",
        "        # DP-SGD baseline\n",
        "        print(f\"\\n  Running DP-SGD...\")\n",
        "        sgd_accs = []\n",
        "        for seed in SEEDS:\n",
        "            set_seed(seed)\n",
        "            acc = train_dp_sgd(noise_mult, EPOCHS)\n",
        "            sgd_accs.append(acc)\n",
        "            print(f\"    Seed {seed}: {acc:.2f}%\")\n",
        "\n",
        "        sgd_mean, sgd_std = np.mean(sgd_accs), np.std(sgd_accs)\n",
        "        print(f\"  DP-SGD: {sgd_mean:.2f} ± {sgd_std:.2f}%\")\n",
        "\n",
        "        results.append({\n",
        "            'epsilon': eps,\n",
        "            'method': 'DP-SGD',\n",
        "            'mean_acc': sgd_mean,\n",
        "            'std_acc': sgd_std,\n",
        "            'noise_multiplier': noise_mult\n",
        "        })\n",
        "\n",
        "        # Comparison\n",
        "        print(f\"\\n  Comparison for ε={eps}:\")\n",
        "        print(f\"    DP-EGGROLL: {egg_mean:.2f}% ± {egg_std:.2f}%\")\n",
        "        print(f\"    DP-SGD:     {sgd_mean:.2f}% ± {sgd_std:.2f}%\")\n",
        "        gap = egg_mean - sgd_mean\n",
        "        print(f\"    Gap: {gap:+.2f}%\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Save Results\n",
        "    # =========================================================================\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(os.path.join(RESULTS_DIR, \"results.csv\"), index=False)\n",
        "\n",
        "    # =========================================================================\n",
        "    # Generate LaTeX Table for Paper Section 6\n",
        "    # =========================================================================\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"LATEX TABLE FOR PAPER (Section 6)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Get non-private result\n",
        "    np_row = df[df['method'] == 'EGGROLL (non-private)'].iloc[0]\n",
        "\n",
        "    print(\"\"\"\n",
        "\\\\begin{table}[h]\n",
        "\\\\centering\n",
        "\\\\caption{MNIST test accuracy (\\\\%) comparison. Privacy: $(\\\\varepsilon, 10^{-5})$-DP.\n",
        "Hyperparameters: $N=\"\"\" + str(BASE_PARAMS['pop_size']) + \"$, $C=\" + str(BASE_PARAMS['loss_clip']) + \"\"\"$,\n",
        "$m=\"\"\" + str(NOMINAL_BATCH_SIZE) + \"\"\"$.}\n",
        "\\\\label{tab:results}\n",
        "\\\\begin{tabular}{lcc}\n",
        "\\\\toprule\n",
        "\\\\textbf{Method} & \\\\textbf{Test Accuracy} & \\\\textbf{Privacy} \\\\\\\\\n",
        "\\\\midrule\"\"\")\n",
        "\n",
        "    print(f\"Non-private EGGROLL & {np_row['mean_acc']:.1f} $\\\\pm$ {np_row['std_acc']:.1f}\\\\% & None \\\\\\\\\")\n",
        "    print(\"\\\\midrule\")\n",
        "\n",
        "    for eps in TARGET_EPSILONS:\n",
        "        egg_rows = df[(df['epsilon'] == eps) & (df['method'] == 'DP-EGGROLL')]\n",
        "        sgd_rows = df[(df['epsilon'] == eps) & (df['method'] == 'DP-SGD')]\n",
        "\n",
        "        if len(egg_rows) == 0:\n",
        "            continue\n",
        "\n",
        "        egg_row = egg_rows.iloc[0]\n",
        "        sgd_row = sgd_rows.iloc[0]\n",
        "\n",
        "        # Bold the winner\n",
        "        if egg_row['mean_acc'] > sgd_row['mean_acc']:\n",
        "            egg_str = f\"\\\\textbf{{{egg_row['mean_acc']:.1f}}} $\\\\pm$ {egg_row['std_acc']:.1f}\\\\%\"\n",
        "            sgd_str = f\"{sgd_row['mean_acc']:.1f} $\\\\pm$ {sgd_row['std_acc']:.1f}\\\\%\"\n",
        "        else:\n",
        "            egg_str = f\"{egg_row['mean_acc']:.1f} $\\\\pm$ {egg_row['std_acc']:.1f}\\\\%\"\n",
        "            sgd_str = f\"\\\\textbf{{{sgd_row['mean_acc']:.1f}}} $\\\\pm$ {sgd_row['std_acc']:.1f}\\\\%\"\n",
        "\n",
        "        print(f\"DP-EGGROLL & {egg_str} & $\\\\varepsilon={eps}$ \\\\\\\\\")\n",
        "        print(f\"DP-SGD & {sgd_str} & $\\\\varepsilon={eps}$ \\\\\\\\\")\n",
        "        if eps != TARGET_EPSILONS[-1]:\n",
        "            print(\"\\\\midrule\")\n",
        "\n",
        "    print(\"\"\"\\\\bottomrule\n",
        "\\\\end{tabular}\n",
        "\\\\end{table}\"\"\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Generate Plot\n",
        "    # =========================================================================\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.axhline(y=np_row['mean_acc'], color='gray', linestyle='--',\n",
        "                label=f'Non-Private ({np_row[\"mean_acc\"]:.1f}%)')\n",
        "\n",
        "    egg_data = df[df['method'] == 'DP-EGGROLL']\n",
        "    if len(egg_data) > 0:\n",
        "        plt.errorbar(egg_data['epsilon'], egg_data['mean_acc'],\n",
        "                    yerr=egg_data['std_acc'], marker='o', capsize=5,\n",
        "                    label='DP-EGGROLL', color='blue', linewidth=2, markersize=8)\n",
        "\n",
        "    sgd_data = df[df['method'] == 'DP-SGD']\n",
        "    if len(sgd_data) > 0:\n",
        "        plt.errorbar(sgd_data['epsilon'], sgd_data['mean_acc'],\n",
        "                    yerr=sgd_data['std_acc'], marker='s', capsize=5,\n",
        "                    label='DP-SGD', color='red', linewidth=2, markersize=8)\n",
        "\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel(r'Privacy Budget ($\\varepsilon$)', fontsize=12)\n",
        "    plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    plt.title(f'Privacy-Utility Tradeoff on MNIST\\n' +\n",
        "              f'(N={BASE_PARAMS[\"pop_size\"]}, C={BASE_PARAMS[\"loss_clip\"]}, δ={DELTA})')\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, \"privacy_utility.png\"), dpi=300)\n",
        "    plt.savefig(os.path.join(RESULTS_DIR, \"privacy_utility.pdf\"))\n",
        "    print(f\"\\nPlot saved to {RESULTS_DIR}/\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
